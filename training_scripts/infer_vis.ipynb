{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64d6ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import re # For sorting in collect_generated_images\n",
    "import glob # For finding images in collect_generated_images\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage # Renamed to avoid conflict\n",
    "import imageio # For GIF creation\n",
    "from IPython.display import Image as IPImage, display # For displaying GIF\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Add infer.py's directory to sys.path if it's not the current directory ---\n",
    "# If infer.py is in the same directory as the notebook, this is not strictly needed\n",
    "# but good practice if you move things around.\n",
    "# current_notebook_dir = os.getcwd()\n",
    "# if current_notebook_dir not in sys.path:\n",
    "#    sys.path.append(current_notebook_dir)\n",
    "\n",
    "# --- Import components from your infer.py ---\n",
    "try:\n",
    "    from infer import Infer, EncoderStatesDataset, collate_fn, TOKEN2ID, UNIQUE_TOKENS\n",
    "    # If infer.py also has WHICH_MODEL, WHICH_STEP etc. as globals, you could import them too:\n",
    "    # from infer import WHICH_MODEL as INFER_PY_WHICH_MODEL # alias if needed\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from infer.py: {e}\")\n",
    "    print(\"Please ensure infer.py is in the same directory as the notebook, or in sys.path.\")\n",
    "    print(\"Also ensure infer.py defines Infer, EncoderStatesDataset, collate_fn, TOKEN2ID, UNIQUE_TOKENS at the top level.\")\n",
    "    raise\n",
    "\n",
    "# --- Import other necessary custom modules (if not handled by infer.py's imports for the notebook's direct use) ---\n",
    "# CompassEncoder is needed to instantiate the 'merger' before passing it to the Infer class\n",
    "try:\n",
    "    from compass_encoder import CompassEncoder # Or whatever the correct class name is\n",
    "    from lora_diffusion import inject_trainable_lora # if directly used in notebook for loading\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing compass_encoder or lora_diffusion: {e}\")\n",
    "    print(\"Ensure these modules are accessible.\")\n",
    "    raise\n",
    "\n",
    "# --- Notebook-specific Configurations ---\n",
    "# You can override or set these independently of infer.py's main block\n",
    "WHICH_MODEL_NOTEBOOK = \"compass\" # Or your specific model checkpoint name from infer.py\n",
    "WHICH_STEP_NOTEBOOK = 20000           # Or your specific checkpoint step from infer.py\n",
    "KEYWORD_NOTEBOOK = \"notebook_interactive\"\n",
    "NUM_INFERENCE_STEPS_NOTEBOOK = 50     # Number of diffusion steps for notebook runs\n",
    "INFER_BATCH_SIZE_NOTEBOOK = 1         # Batch size for inference (adjust based on GPU memory)\n",
    "NOTEBOOK_TMP_DIR = f\"./tmp_{WHICH_MODEL_NOTEBOOK}_{WHICH_STEP_NOTEBOOK}_{KEYWORD_NOTEBOOK}\"\n",
    "OUTPUT_GIF_FILENAME = f\"output_{KEYWORD_NOTEBOOK}.gif\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU (this will be very slow).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af96273-cc9b-4126-a3e7-7143ac25da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_generated_images(tmp_dir, prompt_text_unused, output_gif_path):\n",
    "    \"\"\"\n",
    "    Collects images from the first subdirectory found in tmp_dir (assumed to be the scene's images)\n",
    "    and creates a GIF.\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    subdirs = [d for d in os.listdir(tmp_dir) if osp.isdir(osp.join(tmp_dir, d))]\n",
    "    if not subdirs:\n",
    "        print(f\"No subdirectories found in {tmp_dir} to collect images from.\")\n",
    "        return False\n",
    "\n",
    "    scene_dir_name = subdirs[0]\n",
    "    scene_dir_path = osp.join(tmp_dir, scene_dir_name)\n",
    "    print(f\"Collecting images for GIF from: {scene_dir_path}\")\n",
    "\n",
    "    for ext in (\"*.jpg\", \"*.png\", \"*.jpeg\"):\n",
    "        image_files.extend(glob.glob(osp.join(scene_dir_path, ext)))\n",
    "    \n",
    "    image_files.sort(key=lambda f: int(re.search(r'(\\d+)\\.(jpg|png|jpeg)$', f).group(1)) if re.search(r'(\\d+)\\.(jpg|png|jpeg)$', f) else f)\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {scene_dir_path}\")\n",
    "        return False\n",
    "\n",
    "    frames = []\n",
    "    for image_file in tqdm(image_files, desc=\"Reading frames for GIF\", leave=False):\n",
    "        try:\n",
    "            frames.append(imageio.imread(image_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {image_file} due to error: {e}\")\n",
    "    \n",
    "    if frames:\n",
    "        os.makedirs(osp.dirname(output_gif_path), exist_ok=True)\n",
    "        imageio.mimsave(output_gif_path, frames, fps=10, loop=0)\n",
    "        print(f\"GIF saved to {output_gif_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No frames collected, GIF not created.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173e6035-749e-410c-9b46-5bb38435ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded args.pkl from training: {'ada': False, 'wandb': True, 'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-2-1', 'run_name': 'stage1_20000', 'project': 'iisc', 'controlnet_prompts_file': '../prompts/prompts_2410.txt', 'root_data_dir': '', 'pretrained_vae_name_or_path': None, 'revision': None, 'tokenizer_name': None, 'instance_data_dir_1subject': '../training_data_2410/ref_imgs_1subject', 'instance_data_dir_2subjects': '../training_data_2410/ref_imgs_2subjects', 'controlnet_data_dir_1subject': '../training_data_2410/controlnet_imgs_1subject', 'controlnet_data_dir_2subjects': '../training_data_2410/controlnet_imgs_2subjects', 'class_data_dir': '../training_data_2410/prior_imgs', 'include_class_in_prompt': True, 'pose_only_embedding': True, 'normalize_merged_embedding': False, 'use_location_conditioning': False, 'attn_bbox_from_class_mean': True, 'use_ref_images': True, 'use_controlnet_images': True, 'text_encoder_bypass': False, 'appearance_skip_connection': False, 'replace_attn_maps': 'class2special_soft', 'penalize_special_token_attn': False, 'special_token_attn_loss_weight': 0.1, 'with_prior_preservation': True, 'prior_loss_weight': 1.0, 'vis_dir': '../multiobject/', 'output_dir': '../ckpts/multiobject/__stage1_20000', 'output_format': 'both', 'seed': 1908, 'resolution': 512, 'center_crop': False, 'color_jitter': True, 'train_unet': True, 'train_text_encoder': False, 'textual_inv': False, 'learn_class_embedding': False, 'online_inference': False, 'inference_batch_size': 4, 'train_batch_size': 1, 'sample_batch_size': 4, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'log_every': 25, 'lora_rank': 4, 'learning_rate': 0.0001, 'learning_rate_text': None, 'learning_rate_mlp': None, 'learning_rate_emb': None, 'learning_rate_merger': 0.0001, 'stage1_steps': 20000, 'stage2_steps': 80000, 'merged_emb_dim': 1024, 'lr_warmup_steps': 0, 'use_8bit_adam': False, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'push_to_hub': False, 'hub_token': None, 'mixed_precision': None, 'local_rank': 0, 'resume_training_state': None, 'resize': True, 'use_xformers': False, 'controlnet_prompts': ['a photo of PLACEHOLDER in a snowy forest, with a gentle snowfall and snow-covered trees', 'a photo of PLACEHOLDER in a vast desert with towering sand dunes and a clear blue sky', 'a photo of PLACEHOLDER in a medieval castle courtyard with ancient stone walls and archways', 'a photo of PLACEHOLDER in a sunflower field under a clear blue sky', 'a photo of PLACEHOLDER in a dense rainforest, with sunlight streaming through the canopy', 'a photo of PLACEHOLDER in a serene Japanese garden, surrounded by cherry blossoms', 'a photo of PLACEHOLDER on a rocky cliff overlooking a vast ocean', 'a photo of PLACEHOLDER by a riverside with wildflowers blooming nearby', \"a photo of PLACEHOLDER at a river's edge with stones scattered around\", 'a photo of PLACEHOLDER in front of the Eiffel Tower at sunset', 'a photo of PLACEHOLDER in a vibrant autumn forest, with orange and red leaves carpeting the ground', 'a photo of PLACEHOLDER in a vast open plain, with golden grasses swaying in the wind and distant mountains on the horizon under a wide, clear sky', 'a photo of PLACEHOLDER on a cobblestone street in a quaint European village, with flower-filled balconies and historic buildings', 'a photo of PLACEHOLDER in a canyon with towering red rock formations, and scattered desert plants growing in the rocky terrain'], 'subjects_combs_1subject': ['helicopter', 'horse', 'jeep', 'lion', 'motorbike', 'ostrich', 'sedan', 'shoe', 'sofa', 'teddy'], 'subjects_combs_2subjects': ['helicopter__helicopter', 'helicopter__horse', 'helicopter__jeep', 'helicopter__lion', 'helicopter__motorbike', 'helicopter__ostrich', 'helicopter__sedan', 'helicopter__sofa', 'horse__helicopter', 'horse__horse', 'horse__jeep', 'horse__lion', 'horse__motorbike', 'horse__ostrich', 'horse__sedan', 'horse__sofa', 'horse__teddy', 'jeep__helicopter', 'jeep__horse', 'jeep__jeep', 'jeep__lion', 'jeep__motorbike', 'jeep__ostrich', 'jeep__sedan', 'jeep__sofa', 'jeep__teddy', 'lion__helicopter', 'lion__horse', 'lion__jeep', 'lion__lion', 'lion__motorbike', 'lion__ostrich', 'lion__sedan', 'lion__sofa', 'lion__teddy', 'motorbike__helicopter', 'motorbike__horse', 'motorbike__jeep', 'motorbike__lion', 'motorbike__motorbike', 'motorbike__ostrich', 'motorbike__sedan', 'motorbike__sofa', 'motorbike__teddy', 'ostrich__helicopter', 'ostrich__horse', 'ostrich__jeep', 'ostrich__lion', 'ostrich__motorbike', 'ostrich__ostrich', 'ostrich__sedan', 'ostrich__sofa', 'ostrich__teddy', 'sedan__helicopter', 'sedan__horse', 'sedan__jeep', 'sedan__lion', 'sedan__motorbike', 'sedan__ostrich', 'sedan__sedan', 'sedan__sofa', 'sedan__teddy', 'sofa__helicopter', 'sofa__horse', 'sofa__jeep', 'sofa__lion', 'sofa__motorbike', 'sofa__ostrich', 'sofa__sedan', 'sofa__sofa', 'sofa__teddy', 'teddy__horse', 'teddy__jeep', 'teddy__lion', 'teddy__motorbike', 'teddy__ostrich', 'teddy__sedan', 'teddy__sofa', 'teddy__teddy'], 'subjects': ['horse', 'jeep', 'lion', 'motorbike', 'ostrich', 'teddy', 'shoe', 'helicopter', 'sedan', 'sofa'], 'n_ref_imgs': {'helicopter': 100, 'horse': 100, 'jeep': 100, 'lion': 100, 'motorbike': 100, 'ostrich': 100, 'sedan': 100, 'shoe': 100, 'sofa': 100, 'teddy': 100, 'helicopter__helicopter': 100, 'helicopter__horse': 100, 'helicopter__jeep': 100, 'helicopter__lion': 100, 'helicopter__motorbike': 100, 'helicopter__ostrich': 100, 'helicopter__sedan': 100, 'helicopter__sofa': 100, 'horse__helicopter': 100, 'horse__horse': 100, 'horse__jeep': 100, 'horse__lion': 100, 'horse__motorbike': 100, 'horse__ostrich': 100, 'horse__sedan': 100, 'horse__sofa': 100, 'horse__teddy': 100, 'jeep__helicopter': 100, 'jeep__horse': 100, 'jeep__jeep': 100, 'jeep__lion': 100, 'jeep__motorbike': 100, 'jeep__ostrich': 100, 'jeep__sedan': 100, 'jeep__sofa': 100, 'jeep__teddy': 100, 'lion__helicopter': 100, 'lion__horse': 100, 'lion__jeep': 100, 'lion__lion': 100, 'lion__motorbike': 100, 'lion__ostrich': 100, 'lion__sedan': 100, 'lion__sofa': 100, 'lion__teddy': 100, 'motorbike__helicopter': 100, 'motorbike__horse': 100, 'motorbike__jeep': 100, 'motorbike__lion': 100, 'motorbike__motorbike': 100, 'motorbike__ostrich': 100, 'motorbike__sedan': 100, 'motorbike__sofa': 100, 'motorbike__teddy': 100, 'ostrich__helicopter': 100, 'ostrich__horse': 100, 'ostrich__jeep': 100, 'ostrich__lion': 100, 'ostrich__motorbike': 100, 'ostrich__ostrich': 100, 'ostrich__sedan': 100, 'ostrich__sofa': 100, 'ostrich__teddy': 100, 'sedan__helicopter': 100, 'sedan__horse': 100, 'sedan__jeep': 100, 'sedan__lion': 100, 'sedan__motorbike': 100, 'sedan__ostrich': 100, 'sedan__sedan': 100, 'sedan__sofa': 100, 'sedan__teddy': 100, 'sofa__helicopter': 100, 'sofa__horse': 100, 'sofa__jeep': 100, 'sofa__lion': 100, 'sofa__motorbike': 100, 'sofa__ostrich': 100, 'sofa__sedan': 100, 'sofa__sofa': 100, 'sofa__teddy': 100, 'teddy__horse': 100, 'teddy__jeep': 100, 'teddy__lion': 100, 'teddy__motorbike': 100, 'teddy__ostrich': 100, 'teddy__sedan': 100, 'teddy__sofa': 100, 'teddy__teddy': 100}, 'max_train_steps': 100001}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4189558ac1e849eeb0c84074e1d1b671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training state from: ../ckpts/multiobject/__compass/training_state_20000.pth\n",
      "Injecting and loading LoRA weights for UNet...\n",
      "UNet LoRA Load: Missing keys: 686, Unexpected keys: 0\n",
      "Merger (Pose MLP) weights loaded.\n",
      "Infer class (from infer.py) initialized and models loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Path to your trained model artifacts ---\n",
    "CKPT_BASE_PATH = \"../ckpts/multiobject/\" # Adjust if your ckpt layout is different\n",
    "\n",
    "args_path = osp.join(CKPT_BASE_PATH, f\"__{WHICH_MODEL_NOTEBOOK}\", \"args.pkl\")\n",
    "training_state_path = osp.join(CKPT_BASE_PATH, f\"__{WHICH_MODEL_NOTEBOOK}\", f\"training_state_{WHICH_STEP_NOTEBOOK}.pth\")\n",
    "\n",
    "if not osp.exists(args_path):\n",
    "    raise FileNotFoundError(f\"args.pkl not found at {args_path}. Please check WHICH_MODEL_NOTEBOOK and paths.\")\n",
    "if not osp.exists(training_state_path):\n",
    "    raise FileNotFoundError(f\"Training state .pth not found at {training_state_path}. Please check WHICH_MODEL_NOTEBOOK, WHICH_STEP_NOTEBOOK and paths.\")\n",
    "\n",
    "with open(args_path, \"rb\") as f:\n",
    "    loaded_args_notebook = pickle.load(f) # These are the args from the training run\n",
    "    print(\"Loaded args.pkl from training:\", loaded_args_notebook)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    loaded_args_notebook.get(\"pretrained_model_name_or_path\", \"stabilityai/stable-diffusion-2-1\"), # Use model from args if specified\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "# Initialize merger (Pose Embedding MLP)\n",
    "merger = CompassEncoder(loaded_args_notebook['merged_emb_dim']) # merged_emb_dim from loaded_args\n",
    "\n",
    "print(f\"Loading training state from: {training_state_path}\")\n",
    "training_state = torch.load(training_state_path, map_location=\"cpu\")\n",
    "\n",
    "# Load LoRA weights for UNet if trained (using logic from infer.py's main)\n",
    "if loaded_args_notebook.get('train_unet', False):\n",
    "    print(\"Injecting and loading LoRA weights for UNet...\")\n",
    "    # Assuming inject_trainable_lora expects target_replace_module, you might need to pass it if it's in loaded_args_notebook\n",
    "    # For simplicity, using a common default. Adjust if your lora_diffusion.py has different needs.\n",
    "    target_modules = loaded_args_notebook.get('lora_target_modules', [\"CrossAttention\", \"Attention\", \"GEGLU\"]) # Example, check your args\n",
    "    \n",
    "    # Check if inject_trainable_lora is from the original lora_diffusion or your modified one\n",
    "    # The original from cloneofsimo might not return unet_lora_params\n",
    "    # Assuming your version of inject_trainable_lora behaves as expected\n",
    "    try:\n",
    "        # If your inject_trainable_lora is the one that modifies the model in-place\n",
    "        # and you load weights directly using pipeline.unet.load_state_dict\n",
    "        inject_trainable_lora(pipeline.unet, r=loaded_args_notebook['lora_rank'], target_replace_module=target_modules)\n",
    "    except TypeError as te:\n",
    "        print(f\"TypeError during inject_trainable_lora: {te}. This might be due to API differences.\")\n",
    "        print(\"Ensure inject_trainable_lora and its usage match your lora_diffusion.py version.\")\n",
    "        # Fallback or specific handling if needed\n",
    "\n",
    "    unet_state_dict = pipeline.unet.state_dict()\n",
    "    pretrained_unet_lora_state_dict = {}\n",
    "    if \"unet\" in training_state and \"lora\" in training_state[\"unet\"]:\n",
    "         pretrained_unet_lora_state_dict = training_state[\"unet\"][\"lora\"]\n",
    "    \n",
    "    # Load only LoRA parameters by checking keys\n",
    "    final_unet_state_dict_to_load = {}\n",
    "    for name, param in pretrained_unet_lora_state_dict.items():\n",
    "        if name in unet_state_dict : # Make sure the key exists in the current UNet\n",
    "            final_unet_state_dict_to_load[name] = param\n",
    "        # if name.find(\"lora\") != -1: # Original check from infer.py\n",
    "        #    final_unet_state_dict_to_load[name] = param\n",
    "            \n",
    "    if final_unet_state_dict_to_load:\n",
    "        missing, unexpected = pipeline.unet.load_state_dict(final_unet_state_dict_to_load, strict=False)\n",
    "        print(f\"UNet LoRA Load: Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}\")\n",
    "        if unexpected: print(f\"  Unexpected UNet keys: {unexpected[:5]}...\") # Print a few if many\n",
    "    else:\n",
    "        print(\"Warning: No LoRA weights found or prepared for UNet in training_state.\")\n",
    "else:\n",
    "    print(\"UNet LoRA training was not enabled in args. Skipping UNet LoRA loading.\")\n",
    "\n",
    "\n",
    "# Load merger state\n",
    "if \"merger\" in training_state and \"model\" in training_state[\"merger\"]:\n",
    "    merger.load_state_dict(training_state[\"merger\"][\"model\"], strict=False)\n",
    "    print(\"Merger (Pose MLP) weights loaded.\")\n",
    "else:\n",
    "    print(\"Warning: No merger weights found for 'merger.model' in training_state.\")\n",
    "\n",
    "merger = merger.to(device).eval()\n",
    "pipeline.unet.eval()\n",
    "pipeline.vae.eval()\n",
    "pipeline.text_encoder.eval()\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Create Infer instance (imported from infer.py)\n",
    "# The Infer class from your infer.py should have an __init__ method that matches these arguments.\n",
    "infer_instance_notebook = Infer(\n",
    "    merged_emb_dim=loaded_args_notebook['merged_emb_dim'],\n",
    "    accelerator=accelerator,\n",
    "    unet=pipeline.unet,\n",
    "    scheduler=pipeline.scheduler,\n",
    "    vae=pipeline.vae,\n",
    "    text_encoder=pipeline.text_encoder,\n",
    "    tokenizer=pipeline.tokenizer,\n",
    "    merger=merger,\n",
    "    tmp_dir=NOTEBOOK_TMP_DIR, # Use notebook-specific temp directory\n",
    "    bs=INFER_BATCH_SIZE_NOTEBOOK # Use notebook-specific batch size\n",
    ")\n",
    "# Pass NUM_INFERENCE_STEPS_NOTEBOOK to the call method if it's used there\n",
    "# The Infer class provided in the prompt uses a global NUM_INFERENCE_STEPS\n",
    "\n",
    "print(\"Infer class (from infer.py) initialized and models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce93d5f-d11d-4e3e-a47b-71ffb84399ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames to generate: 8\n",
      "Scene setup: 2 subjects.\n",
      "Example TOKEN2ID['sedan']: 24237\n",
      "Example UNIQUE_TOKENS['0_0']: bnha\n"
     ]
    }
   ],
   "source": [
    "num_frames_for_gif = 8 # Number of frames for the animation\n",
    "\n",
    "# --- Define Bounding Boxes ---\n",
    "bbox_subject1_static = [[0.00, 0.55, 0.45, 1.00]] * (num_frames_for_gif + 1)\n",
    "bbox_subject2_static = [[0.55, 0.55, 1.00, 1.00]] * (num_frames_for_gif + 1)\n",
    "\n",
    "# --- Define Scene Data ---\n",
    "current_scene_subjects_data = [\n",
    "    {\n",
    "        \"subject\": \"sedan\", # Make sure 'sedan' is in TOKEN2ID (imported from infer.py)\n",
    "        \"normalized_azimuths\": np.linspace(0, 1, num_frames_for_gif + 1),\n",
    "        \"bboxes\": bbox_subject1_static\n",
    "    },\n",
    "    {\n",
    "        \"subject\": \"suv\",   # Make sure 'suv' is in TOKEN2ID (imported from infer.py)\n",
    "        \"normalized_azimuths\": 1 - np.linspace(0, 1, num_frames_for_gif + 1),\n",
    "        \"bboxes\": bbox_subject2_static\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_for_scene = \"cinematic film still of PLACEHOLDER in the backyard of a bungalow on a sunny afternoon, sharp, high quality, best quality, high resolution, 8k\"\n",
    "\n",
    "print(f\"Number of frames to generate: {num_frames_for_gif}\")\n",
    "print(f\"Scene setup: {len(current_scene_subjects_data)} subjects.\")\n",
    "# TOKEN2ID and UNIQUE_TOKENS are now from infer.py\n",
    "print(f\"Example TOKEN2ID['sedan']: {TOKEN2ID.get('sedan', 'Not in TOKEN2ID')}\")\n",
    "print(f\"Example UNIQUE_TOKENS['0_0']: {UNIQUE_TOKENS.get('0_0', 'Not in UNIQUE_TOKENS')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba1c35-4c54-4856-94f4-8beae011b777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference for prompt: cinematic film still of PLACEHOLDER in the backyard of a bungalow on a sunny afternoon, sharp, high quality, best quality, high resolution, 8k\n",
      "Output GIF will be saved to: output_notebook_interactive.gif\n",
      "every thread finished generating the encoder hidden states...\n",
      "every thread finished preparing their dataloaders...\n",
      "starting generation...\n",
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/000.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/contwords/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/001.jpg']\n",
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/002.jpg']\n",
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/003.jpg']\n",
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/004.jpg']\n",
      "0 is doing ['./tmp_compass_20000_notebook_interactive/sedan__suv/005.jpg']\n"
     ]
    }
   ],
   "source": [
    "if osp.exists(OUTPUT_GIF_FILENAME):\n",
    "    os.remove(OUTPUT_GIF_FILENAME)\n",
    "if osp.exists(NOTEBOOK_TMP_DIR): # Clean tmp dir before run\n",
    "    shutil.rmtree(NOTEBOOK_TMP_DIR)\n",
    "os.makedirs(NOTEBOOK_TMP_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"Starting inference for prompt: {prompt_for_scene}\")\n",
    "print(f\"Output GIF will be saved to: {OUTPUT_GIF_FILENAME}\")\n",
    "\n",
    "# The `__call__` method of the Infer class (from infer.py) will be invoked.\n",
    "# It needs `args` which are the loaded training arguments.\n",
    "# Also, NUM_INFERENCE_STEPS is used by the Infer class. If it's a global in infer.py,\n",
    "# you might need to ensure the notebook's value is used, or modify Infer to accept it.\n",
    "# For now, assuming Infer class uses a global NUM_INFERENCE_STEPS or it's hardcoded.\n",
    "# The provided Infer class structure has NUM_INFERENCE_STEPS hardcoded/globally accessible in its methods.\n",
    "# Let's ensure the 'args' dict passed to __call__ is the one from training.\n",
    "# And the Infer class from infer.py uses global constants for NUM_INFERENCE_STEPS, etc.\n",
    "# If your Infer class in infer.py was modified to take NUM_INFERENCE_STEPS in __init__ or __call__, adjust accordingly.\n",
    "\n",
    "infer_instance_notebook(\n",
    "    seed=42,\n",
    "    gif_path=OUTPUT_GIF_FILENAME,\n",
    "    prompt=prompt_for_scene,\n",
    "    all_subjects_data=[current_scene_subjects_data],\n",
    "    args=loaded_args_notebook # Pass the loaded_args from training, as Infer class expects this structure\n",
    ")\n",
    "\n",
    "# Display the generated GIF\n",
    "if osp.exists(OUTPUT_GIF_FILENAME):\n",
    "    print(\"Displaying generated GIF:\")\n",
    "    display(IPImage(filename=OUTPUT_GIF_FILENAME))\n",
    "else:\n",
    "    print(f\"GIF not found at {OUTPUT_GIF_FILENAME}. Check logs in {NOTEBOOK_TMP_DIR} and console output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174742f5-9bc0-4c5b-90cf-76fb2e30f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72bce2-5319-4b56-8ae5-be0f5098fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca561bcf-6763-4784-960a-8fcae8ceeed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
